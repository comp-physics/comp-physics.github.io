---
title: "Hardware"
layout: gridlay
sitemap: false
permalink: /hardware/
---

## Computational Resources

The Computational Physics Group maintains several local servers and access to supercomputers around the country.
This page gives a brief overview of servers we marshal for our work.

### In house

<div class="jumbotron">

#### Group-owned

* 3x, "Instinct", Dual AMD EPYC 7713 Milan, 2x AMD MI210 80GB
* 1x, "Wingtip3", Dual Intel 6338, 5x NVIDIA A100 80GB
* 4x, NVIDIA Bluefield 2 DPUs

#### COC/PACE ICE

GPU nodes
* 10x, Dual Xeon Gold 6226, 4x NVIDIA Quadro Pro RTX6000 24GB
* 11x, Dual Xeon Gold 6226, 2x NVIDIA V100 16GB
* 4x,	Dual Xeon Gold 6248, 1x NVIDIA V100 32GB
* 4x, Xeon Gold 6248, 4x NVIDIA V100 32GB
* 2x,	Dual AMD EPYC 7452,	2x NVIDIA A40 48GB
* 2x,	Dual AMD EPYC 7513,	2x NVIDIA A100 40GB
* 2x,	Dual AMD EPYC 7452,	2x NVIDIA A100 80GB
* 3x, "Newell", Power9 O2CY417, 2x NVIDIA V100 32GB

CPU nodes
* 60x, Dual Xeon Gold 6226 
* 4x,	Dual AMD EPYC 7513

Find more information [here](https://docs.pace.gatech.edu/ice_cluster/ice/).

#### The Rouges 

We work with [CRNCH](https://crnch.gatech.edu/), the Center for Research into Novel Computing Hierarchies.
CRNCH maintains the [Rogues Gallery](https://gt-crnch-rg.readthedocs.io/en/main/general/rg-hardware.html), a collection of (mostly) non-von Neumann systems. 
Here, we work with ARM servers, FPGAs, RISC-V boards, Neuromorphic nodes, near-memory nodes (Pathfinder), and reconfigurable networked machines.
Check out the link above for to learn more about this ever-evolving list of systems.

#### Phoenix

We maintain allocations on Georgia Tech's [Phoenix](https://docs.pace.gatech.edu/phoenix_cluster/gettingstarted_phnx/), a Top500 supercomputer managed by [PACE](https://pace.gatech.edu/).
Phoenix houses a variety of CPU and GPU nodes.
You can see what is available [here](https://docs.pace.gatech.edu/phoenix_cluster/resources_phnx/), though it's not always fully up-to-date.
In December 2022, Phoenix housed just over 1000 CPU nodes and 100 GPU nodes.

</div>

### Outside resources

<div class="jumbotron">

We maintain allocations on clusters and leadership class supercomputers around the country. 
This includes leadership-class DOE systems like
* [OLCF Summit (Top500 #5)](https://www.olcf.ornl.gov/summit/)
* [OLCF Crusher (Frontier early-readiness system, Top500 #1)](https://docs.olcf.ornl.gov/systems/crusher_quick_start_guide.html)
* [OLCF Wombat](https://www.olcf.ornl.gov/olcf-resources/compute-systems/wombat/)

as well as university systems like
* [PSC Bridges2](https://www.psc.edu/resources/bridges-2/)
* [SDSC Expanse](https://www.sdsc.edu/services/hpc/expanse/)
* [TACC Stampede2](https://portal.tacc.utexas.edu/user-guides/stampede2)

</div>
