---
title: "Hardware"
layout: gridlay
sitemap: false
permalink: /hardware/
---

## Computational Resources

The Computational Physics Group maintains several local servers and access to supercomputers and quantum computers around the country.
This page gives a brief overview of the computing resources we investigate and marshal for our work.

### In house

<div class="jumbotron">

#### Group-owned

* 3x, "Instinct", Dual AMD EPYC 7713 Milan, 2x AMD MI210 80GB
* 1x, "Wingtip3", Dual Intel 6338, 5x NVIDIA A100 80GB
* 4x, NVIDIA Bluefield 2 DPUs

#### COC/PACE ICE

GPU nodes
* 10x, Dual Xeon Gold 6226, 4x NVIDIA Quadro Pro RTX6000 24GB
* 11x, Dual Xeon Gold 6226, 2x NVIDIA V100 16GB
* 4x,	Dual Xeon Gold 6248, 1x NVIDIA V100 32GB
* 4x, Xeon Gold 6248, 4x NVIDIA V100 32GB
* 2x,	Dual AMD EPYC 7452,	2x NVIDIA A40 48GB
* 2x,	Dual AMD EPYC 7513,	2x NVIDIA A100 40GB
* 2x,	Dual AMD EPYC 7452,	2x NVIDIA A100 80GB
* 3x, "Newell", Power9 O2CY417, 2x NVIDIA V100 32GB

CPU nodes
* 60x, Dual Xeon Gold 6226 
* 4x,	Dual AMD EPYC 7513

Find more information [here](https://docs.pace.gatech.edu/ice_cluster/ice/).

#### The Rogues 

We work with [CRNCH](https://crnch.gatech.edu/), the Center for Research into Novel Computing Hierarchies.
CRNCH maintains the [Rogues Gallery](https://gt-crnch-rg.readthedocs.io/en/main/general/rg-hardware.html), a collection of (mostly) non-von Neumann systems. 
Here, we work with 
* ARM servers (NVIDIA ARM HPC Dev Kits)
* FPGAs (Xilinx Alveo U50, U250, U280, AC-510)
* RISC-V boards (MangoPi)
* The first RISC-V GPGPU (called [Vortex](https://vortex.cc.gatech.edu/))
* Neuromorphic nodes
* Near-memory servers ([Pathfinder](https://lucata.com/solutions/pathfinder/)),
* Reconfigurable networked machines, including
	* FPGA-based network devices
	* CPU-based smart network devices
	* 5G networking testbed equipment

Check out the links above to learn more about this ever-evolving list of systems.

#### Phoenix

We maintain allocations on Georgia Tech's [Phoenix](https://docs.pace.gatech.edu/phoenix_cluster/gettingstarted_phnx/), a Top500 supercomputer managed by [PACE](https://pace.gatech.edu/).
Phoenix houses a variety of CPU and GPU nodes.
You can [see the available resources here](https://docs.pace.gatech.edu/phoenix_cluster/resources_phnx/), though they are not always up-to-date.
In December 2022, Phoenix housed just over 1000 CPU nodes and 100 GPU nodes.

</div>

### Outside resources

<div class="jumbotron">

We maintain allocations on clusters and leadership class supercomputers around the country. 
This includes leadership-class DOE systems like
* [OLCF Summit (Top500 #5)](https://www.olcf.ornl.gov/summit/)
* [OLCF Crusher (Frontier early-readiness system, Top500 #1)](https://docs.olcf.ornl.gov/systems/crusher_quick_start_guide.html)
* [OLCF Wombat](https://www.olcf.ornl.gov/olcf-resources/compute-systems/wombat/)

We use university supercomputers like
* [PSC Bridges2](https://www.psc.edu/resources/bridges-2/)
* [SDSC Expanse](https://www.sdsc.edu/services/hpc/expanse/)
* [TACC Stampede2](https://portal.tacc.utexas.edu/user-guides/stampede2)
</div>

## Quantum computers

<div class="jumbotron">
We also work on quantum algorithms for solving scientific problems, often based on continuum physics.
For this, we use quantum simulation (on the classical hardware above) and quantum computers from
* [Oak Ridge QCUP](https://www.olcf.ornl.gov/olcf-resources/compute-systems/quantum-computing-user-program/)
* [IBM Quantum](https://quantum-computing.ibm.com/services/resources/docs/resources/manage/systems/)
* and more pending

</div>
