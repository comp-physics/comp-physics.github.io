---
title: "Hardware"
layout: gridlay
sitemap: false
permalink: /hardware/
---

## Computational Resources

The group maintains several local servers and access to supercomputers and quantum computers around the country.
This page gives a summary of the computing resources we marshal for our work.

### In house

<div class="jumbotron">

#### Group procured

* 4x, [NVIDIA GraceHopper Superchip](https://www.nvidia.com/en-us/data-center/grace-hopper-superchip/) nodes
* 1x, [NVIDIA H100](https://www.nvidia.com/en-us/data-center/h100/) GPU
* 3x, "[Instinct](https://www.cc.gatech.edu/news/new-hardware-brings-students-closer-exascale-computing)", Dual AMD EPYC 7713 Milan, 2x AMD MI210 64GB
* 1x, "Wingtip-GPU", Dual Intel 6338, 5x NVIDIA A100 80GB
* 4x, [NVIDIA Bluefield 2](https://resources.nvidia.com/en-us-accelerated-networking-resource-library/bluefield-2-dpu-datasheet?lx=LbHvpR&topic=networking-cloud) DPUs

#### Rogues Gallery

We work with [CRNCH](https://crnch.gatech.edu/), the Center for Research into Novel Computing Hierarchies.
CRNCH maintains the [Rogues Gallery](https://gt-crnch-rg.readthedocs.io/en/main/general/rg-hardware.html), a collection of (mostly) non-von Neumann systems.
A full hardware list is [held here](https://gt-crnch-rg.readthedocs.io/en/main/general/rg-hardware.html).
It includes Arm servers, FPGAs, RISC-V boards, the first RISC-V GPGPU (called [Vortex](https://vortex.cc.gatech.edu/)), neuromorphic nodes, near-memory servers (like [Pathfinder](https://lucata.com/solutions/pathfinder/)), and more.

#### PACE Phoenix

We maintain allocations on Georgia Tech's Phoenix, a Top500 supercomputer managed by [PACE](https://pace.gatech.edu/).
Phoenix houses a variety of CPU and GPU nodes.
In December 2023, Phoenix housed just over 1000 CPU nodes and 100 GPU nodes.

#### ICE

[GT ICE](https://docs.pace.gatech.edu/ice_cluster/ice/) holds, at time of writing, about 50 GPU nodes and 100 CPU nodes of various flavors.
The specific resources are listed [here](https://gatech.service-now.com/home?id=kb_article_view&sysparm_article=KB0042095).

</div>

### Outside resources

<div class="jumbotron">
We maintain allocations on clusters and leadership class supercomputers around the country. 
This includes leadership-class DOE systems and early access systems like
* [LLNL Tioga](https://hpc.llnl.gov/hardware/compute-platforms/tioga), El Capitan testbed, AMD MI300A-based
* [OLCF Frontier (Top500 #1)](https://www.olcf.ornl.gov/frontier/), Exascale, AMD MI250X-based
* [OLCF Summit (Top500 #5)](https://www.olcf.ornl.gov/summit/), NVIDIA V100-based
* [OLCF Wombat](https://www.olcf.ornl.gov/olcf-resources/compute-systems/wombat/), Arm+GPU+SmartNIC testbed

We use supercomputers at other university centers like
* [NCSA Delta](https://delta.ncsa.illinois.edu/)
* [PSC Bridges2](https://www.psc.edu/resources/bridges-2/)
* [SDSC Expanse](https://www.sdsc.edu/services/hpc/expanse/)
</div>

## Quantum computers

<div class="jumbotron">
We also work on quantum algorithms for solving scientific problems, often based on continuum physics.
For this, we use quantum simulation (on the classical hardware above) and quantum computers from
* [Oak Ridge QCUP](https://www.olcf.ornl.gov/olcf-resources/compute-systems/quantum-computing-user-program/)
* [IBM Quantum](https://quantum-computing.ibm.com/services/resources/docs/resources/manage/systems/)
* [Quantinuum](https://www.quantinuum.com/)
</div>
